{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, HTML\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_CSVS = 'data/raw'\n",
    "RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détecte automatiquement le délimiteur d'un fichier CSV\n",
    "def get_delimiter(file_path, bytes=4096):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = file.read(bytes)\n",
    "        sniffer = csv.Sniffer()\n",
    "        delimiter = sniffer.sniff(data).delimiter\n",
    "        return delimiter\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erreur lors de la détection du délimiteur: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture d'un fichier CSV en essayant différents encodages\n",
    "def read_csv_file(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        return None, False, f\"Fichier non trouvé: {file_path}\"\n",
    "    \n",
    "    delimiter = get_delimiter(file_path)\n",
    "    if not delimiter:\n",
    "        return None, False, f\"Impossible de détecter le délimiteur pour le fichier: {file_path}\"\n",
    "    \n",
    "    encodings = ['utf-8', 'latin1', 'ISO-8859-1']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, low_memory=False, encoding=encoding, delimiter=delimiter)\n",
    "            return df, True, None\n",
    "        except (UnicodeDecodeError, pd.errors.ParserError) as e:\n",
    "            logging.warning(f\"Erreur avec l'encodage {encoding} pour le fichier {file_path}: {e}\")\n",
    "    \n",
    "    return None, False, f\"Impossible de lire le fichier {file_path} avec les encodages: {encodings}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des datasets dans des dictionnaires\n",
    "def load_datasets(prefixes, years, base_path=PATH_TO_CSVS):\n",
    "    dataframes = {}\n",
    "    \n",
    "    for prefix in prefixes:\n",
    "        datasets = {}\n",
    "        for year in years:\n",
    "            connector = '_' if year <= 2016 else '-'\n",
    "            file_name = os.path.join(base_path, f'{prefix}{connector}{year}.csv')\n",
    "            df, success, error = read_csv_file(file_name)\n",
    "            if success:\n",
    "                datasets[year] = df\n",
    "            else:\n",
    "                logging.error(error)\n",
    "        dataframes[prefix] = datasets\n",
    "    \n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-25 14:16:48,519 - INFO - caracteristiques: 4 datasets chargés.\n",
      "2024-11-25 14:16:48,520 - INFO - lieux: 4 datasets chargés.\n",
      "2024-11-25 14:16:48,520 - INFO - usagers: 4 datasets chargés.\n",
      "2024-11-25 14:16:48,521 - INFO - vehicules: 4 datasets chargés.\n",
      "2024-11-25 14:16:48,521 - INFO - Total datasets chargés: 16.\n"
     ]
    }
   ],
   "source": [
    "years = list(range(2019, 2023))\n",
    "prefixes = ['caracteristiques', 'lieux', 'usagers', 'vehicules']\n",
    "\n",
    "dataframes = load_datasets(prefixes, years)\n",
    "\n",
    "# log\n",
    "for prefix, df_dict in dataframes.items():\n",
    "    logging.info(f'{prefix}: {len(df_dict)} datasets chargés.')\n",
    "\n",
    "logging.info(f'Total datasets chargés: {sum(len(df_dict) for df_dict in dataframes.values())}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir les dtypes d'un DataFrame\n",
    "def convert_dtypes(df, reference_dtypes):\n",
    "    for col in df.columns:\n",
    "        if col in reference_dtypes:\n",
    "            try:\n",
    "                df[col] = df[col].astype(reference_dtypes[col])\n",
    "            except ValueError as e:\n",
    "                logging.error(f\"Erreur lors de la conversion de la colonne {col} en type {reference_dtypes[col]}: {e}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire la structure de référence de chaque groupe de DataFrames\n",
    "def extract_reference_structure(dataframes):\n",
    "    reference_structures = {}\n",
    "    \n",
    "    for prefix, df_dict in dataframes.items():\n",
    "        if df_dict:\n",
    "            last_df = list(df_dict.values())[-1]\n",
    "            reference_structures[prefix] = last_df.dtypes.to_dict()\n",
    "    return reference_structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gérer le préprocessing des DataFrames\n",
    "def preprocess(df, prefix):\n",
    "    if prefix == 'caracteristiques':\n",
    "        if 'Accident_Id' in df.columns:\n",
    "            df = df.rename(columns={'Accident_Id': 'Num_Acc'})\n",
    "        if 'an' in df.columns:\n",
    "            df['an'] = df['an'].apply(lambda x: x + 2000 if x < 2000 else x)\n",
    "        if 'hrmn' in df.columns:\n",
    "            df['hrmn'] = df['hrmn'].apply(lambda x: f\"{str(x).zfill(4)[:2]}:{str(x).zfill(4)[2:]}\")\n",
    "        df = df.fillna({'lum': -1, 'int': -1, 'atm': -1, 'col': -1})\n",
    "        df = df.drop(columns=['adr', 'lat', 'long'], errors='ignore')\n",
    "    elif prefix == 'lieux':\n",
    "        df = df.fillna({'circ': -1, 'vosp': -1, 'prof': -1, 'pr': -1, 'pr1': -1, 'plan': -1, 'surf': -1, 'infra': -1, 'situ': -1})\n",
    "        if 'lartpc' in df.columns:\n",
    "            df['lartpc'] = df['lartpc'].replace(0, -1).fillna(-1)\n",
    "        if 'vma' in df.columns:\n",
    "            df['vma'] = df['vma'].apply(lambda x: -1 if pd.isna(x) or x > 130 else x)\n",
    "        df = df.drop(columns=['voie', 'v1', 'v2', 'larrout'], errors='ignore')\n",
    "    elif prefix == 'usagers':\n",
    "        df = df.fillna({'place': -1, 'catu': -1, 'grav': -1, 'sexe': -1, 'trajet': -1, 'secu1': -1, 'secu2': -1, 'secu3': -1, 'locp': -1, 'actp': -1, 'etatp': -1})\n",
    "        if 'catu' in df.columns:\n",
    "            df['catu'] = df['catu'].replace(4, -1)\n",
    "        if 'an_nais' in df.columns:\n",
    "            df['an_nais'] = df['an_nais'].apply(lambda x: pd.NA if x < 1900 else x)\n",
    "    elif prefix == 'vehicules':\n",
    "        df = df.fillna({'senc': -1, 'obs': -1, 'obsm': -1, 'choc': -1, 'manv': -1, 'motor': -1})\n",
    "        if 'catv' in df.columns:\n",
    "            df['catv'] = df['catv'].fillna(0)\n",
    "        df = df.drop(columns=['occutc'], errors='ignore')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer le préprocessing sur tous les datasets\n",
    "def preprocess_datasets(dataframes):\n",
    "    for prefix, df_dict in dataframes.items():\n",
    "        for year, df in df_dict.items():\n",
    "            df = preprocess(df, prefix)\n",
    "            df_dict[year] = df\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harmoniser les DataFrames selon la structure de référence\n",
    "def harmonize_dataframes(dataframes, reference_structures):\n",
    "    harmonized_dataframes = {}\n",
    "    for prefix, df_dict in dataframes.items():\n",
    "        reference_dtypes = reference_structures.get(prefix, {})\n",
    "        harmonized_dict = {}\n",
    "        for year, df in df_dict.items():\n",
    "            df = df[[col for col in df.columns if col in reference_dtypes]]\n",
    "            df = convert_dtypes(df, reference_dtypes)\n",
    "            harmonized_dict[year] = df\n",
    "        harmonized_dataframes[prefix] = harmonized_dict\n",
    "    return harmonized_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = preprocess_datasets(dataframes)\n",
    "reference_structures = extract_reference_structure(dataframes)\n",
    "harmonized_dataframes = harmonize_dataframes(dataframes, reference_structures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour concaténer les DataFrames par type\n",
    "def concat_harmonized_dataframes(dataframes):\n",
    "    concatenated_dataframes = {}\n",
    "    for prefix, df_dict in dataframes.items():\n",
    "        concatenated_df = pd.concat(df_dict.values(), ignore_index=True)\n",
    "        concatenated_dataframes[prefix] = concatenated_df\n",
    "    return concatenated_dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_dataframes = concat_harmonized_dataframes(harmonized_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusionner les DataFrames\n",
    "\n",
    "def merge_dataframes(concatenated_dataframes):\n",
    "    df_caracteristiques = concatenated_dataframes['caracteristiques']\n",
    "    df_lieux = concatenated_dataframes['lieux']\n",
    "    df_usagers = concatenated_dataframes['usagers']\n",
    "    df_vehicules = concatenated_dataframes['vehicules']\n",
    "    \n",
    "    merged_df = pd.merge(df_caracteristiques, df_lieux, on='Num_Acc', how='inner')\n",
    "    merged_df = pd.merge(merged_df, df_usagers, on='Num_Acc', how='inner')\n",
    "    merged_df = pd.merge(merged_df, df_vehicules, on=['Num_Acc', 'id_vehicule', 'num_veh'], how='inner')\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_df = merge_dataframes(concatenated_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement après fusion\n",
    "def preprocessing_final_dataframe(df):\n",
    "    df = df.drop(columns=['id_usager', 'Num_Acc', 'com', 'id_vehicule', 'num_veh', 'lartpc'], errors='ignore')\n",
    "    if 'hrmn' in df.columns:\n",
    "        df['hour'] = df['hrmn'].str[:2].astype(int)\n",
    "        df = df.drop(columns=['hrmn'])\n",
    "    if 'an_nais' in df.columns:\n",
    "        mode_an_nais = df['an_nais'].mode()[0]\n",
    "        df['an_nais'] = df['an_nais'].fillna(mode_an_nais).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage du DataFrame\n",
    "def encode_dataframe(df):\n",
    "    dummy_columns = ['lum', 'agg', 'int', 'atm', 'col', 'catr', 'circ', 'prof', 'place', 'catu', 'sexe', 'trajet', 'secu1', 'secu2', 'secu3', 'locp', 'actp', 'etatp', 'senc', 'catv', 'obs', 'obsm', 'choc', 'manv', 'motor', 'plan', 'surf', 'an', 'infra', 'dep', 'situ', 'vosp']\n",
    "    df = pd.get_dummies(df, columns=dummy_columns, drop_first=True)\n",
    "    return df\n",
    "\n",
    "final_merged_df = preprocessing_final_dataframe(final_merged_df)\n",
    "final_merged_df = encode_dataframe(final_merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrer le DataFrame final\n",
    "PATH_TO_CSVS_PROCESSED = 'data/processed/data.csv'\n",
    "final_merged_df.reset_index(drop=True, inplace=True)\n",
    "final_merged_df.to_csv(PATH_TO_CSVS_PROCESSED, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jour</th>\n",
       "      <th>mois</th>\n",
       "      <th>nbv</th>\n",
       "      <th>pr</th>\n",
       "      <th>pr1</th>\n",
       "      <th>vma</th>\n",
       "      <th>grav</th>\n",
       "      <th>an_nais</th>\n",
       "      <th>hour</th>\n",
       "      <th>lum_1</th>\n",
       "      <th>...</th>\n",
       "      <th>situ_2</th>\n",
       "      <th>situ_3</th>\n",
       "      <th>situ_4</th>\n",
       "      <th>situ_5</th>\n",
       "      <th>situ_6</th>\n",
       "      <th>situ_8</th>\n",
       "      <th>vosp_0</th>\n",
       "      <th>vosp_1</th>\n",
       "      <th>vosp_2</th>\n",
       "      <th>vosp_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>900</td>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "      <td>2002</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>900</td>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "      <td>1993</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>900</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>1959</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>845</td>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "      <td>1994</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>500</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>1996</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 387 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   jour  mois nbv  pr  pr1  vma  grav  an_nais  hour  lum_1  ...  situ_2  \\\n",
       "0    30    11  10   6  900   70     4     2002     1  False  ...   False   \n",
       "1    30    11  10   6  900   70     4     1993     1  False  ...   False   \n",
       "2    30    11  10   6  900   70     1     1959     1  False  ...   False   \n",
       "3    30    11   2   3  845   70     4     1994     2  False  ...   False   \n",
       "4    28    11   8  10  500   90     1     1996    15   True  ...   False   \n",
       "\n",
       "   situ_3  situ_4  situ_5  situ_6  situ_8  vosp_0  vosp_1  vosp_2  vosp_3  \n",
       "0   False   False   False   False   False    True   False   False   False  \n",
       "1   False   False   False   False   False    True   False   False   False  \n",
       "2   False   False   False   False   False    True   False   False   False  \n",
       "3   False   False   False   False   False    True   False   False   False  \n",
       "4   False   False   False   False   False    True   False   False   False  \n",
       "\n",
       "[5 rows x 387 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de doublons : 469\n",
      "Taux de doublons : 0.09%\n",
      "Nombre de valeur manquante : 0\n",
      "Taux de valeur manquante : 0.00%\n"
     ]
    }
   ],
   "source": [
    "duplicate_rows = final_merged_df.duplicated().sum()\n",
    "total_rows = len(final_merged_df)\n",
    "duplicate_rate = duplicate_rows / total_rows * 100\n",
    "\n",
    "print(f\"Nombre de doublons : {duplicate_rows}\")\n",
    "print(f\"Taux de doublons : {duplicate_rate:.2f}%\")\n",
    "\n",
    "missing_values = final_merged_df.isnull().sum().sum()\n",
    "missing_rate = missing_values / total_rows * 100\n",
    "\n",
    "print(f\"Nombre de valeur manquante : {missing_values}\")\n",
    "print(f\"Taux de valeur manquante : {missing_rate:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
